# text-davinci-003, text-curie-001, text-babbage-001, text-ada-001
engine: text-davinci-003

# The prompt to start the generation from.
prompt: '' 

# The maximum number of tokens to generate in the completion.
max_tokens: 100

# Sampling temperature between 0 and 2. Higher values will make the output more random, 
# while lower values like 0.2 will make it more focused and deterministic.
temperature: .75

# An alternative to temperature, nucleus sampling. The model considers the results
# of the toklen with top_p probability mass. So 0.1 means only the tokens comprising
# the top 10% probability mass are considered.
top_p: 1

# Returns the best `n` out of `best_of` completions made on server side
n: 1
best_of: 3

# Whether to stream back partial progress
stream: False

# Include log-probabilities of the top `logprobs` tokens.
logprobs: 0

# up to 4 sequences that stop the generation
stop: '' 

# Dictionary that can modify the likelihood of specified tokens appearing in the completion.
# logit_bias: {}

# Other params 
frequency_penalty: 0
presence_penalty: 0  
request_timeout: 20



